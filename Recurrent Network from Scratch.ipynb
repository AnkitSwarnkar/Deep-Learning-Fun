{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural network in numpy step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN) are one of the powerful tool which can be used to convert sequence data into insight. In this notebook, I am building recurrent network with numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_func(yhat):\n",
    "    \"\"\"\n",
    "    Input vector y of n*1\n",
    "    \"\"\"\n",
    "    #Shifting max to 0 for numerical stability\n",
    "    t = np.exp(yhat-np.max(yhat))\n",
    "    return t/np.sum(t,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a simple rnn cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cell_rnn(a_prev, x_curr, weight_dict):\n",
    "    \"\"\"\n",
    "    Input : a_prev : last layer activation function\n",
    "            x_curr : current input at time t\n",
    "    Output:  a_curr, y_hat_curr\n",
    "    \"\"\"\n",
    "    W_ax = weight_dict[\"W_ax\"]\n",
    "    b_a =  weight_dict[\"b_a\"]\n",
    "    W_aa = weight_dict[\"W_aa\"]\n",
    "    W_ya = weight_dict[\"W_ya\"]\n",
    "    b_y =  weight_dict[\"b_y\"]\n",
    "    a_curr = np.tanh((np.dot(W_ax,x_curr)+np.dot(W_aa,a_prev) + b_a))\n",
    "    y_hat_curr = softmax_func(np.dot(W_ya,a_curr) + b_y)\n",
    "    # store backward propagation\n",
    "    cache = (a_curr, a_prev, x_curr, weight_dict)\n",
    "    return a_curr,y_hat_curr,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN can be viewed as unrolling above cell k times if sequence is of length k. The output a_curr will become a_prev of the next step and this process will continue k times.\n",
    "The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$.\n",
    "\n",
    "We need to store the return value in cache:\n",
    "\n",
    "* next activation at t\n",
    "* previous activation at t\n",
    "* dataX at time t\n",
    "*  parameter at time t\n",
    "\n",
    "<img src=\"input_rnn.PNG\" style=\"width:700px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_rnn(x,a0,weight_dict):\n",
    "    \"\"\"\n",
    "    Input : \n",
    "    x  : sequence data n_x * k * T_x matrix (n_x is number of feature, T_x is time and k is data count)\n",
    "    a0 : activation at zeroth time. Dimension : n_a * k #\n",
    "    weight_dict: Parameters dictionary\n",
    "    weight_dict : All parameters \n",
    "    \n",
    "    Return:\n",
    "    Cache : It will serve as a storage to the backprop. Value to be stored:\n",
    "            1. input x \n",
    "            2. Activation for every step t for every point \n",
    "    y_hat : Dimension : n_x * k * n \n",
    "    a_all : Activation of all hidden state\n",
    "    \"\"\"\n",
    "    cache = [] #To hold values during forward prop for backprop \n",
    "    no_pass = np.shape(x)[1]\n",
    "    #Iniitalize yhat and a_0\n",
    "    n_x = x.shape[0] #no of features\n",
    "    k = x.shape[1] # number of points\n",
    "    T_x = x.shape[2] # no of time points\n",
    "    n_a = a0.shape[0] # activation size x\n",
    "    n_y = weight_dict[\"W_ya\"].shape[0] #final output size\n",
    "    y_hat_all = np.zeros((n_y,k,T_x))\n",
    "    a_all  = np.zeros((n_a,k,T_x))\n",
    "    #Initial value of a_cur which is a<t>\n",
    "    a_cur = a0\n",
    "    for count_t in range(T_x):\n",
    "        current_x = x[:,:,count_t]\n",
    "        a_cur, yhat, cache_return = cell_rnn(a_cur,current_x,weight_dict)\n",
    "        y_hat_all[:,:,count_t] = yhat\n",
    "        a_all[:,:,count_t] = a_cur\n",
    "        cache.append(cache_return)\n",
    "    caches = (cache, x)\n",
    "    return  a_all,y_hat_all,caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The cool name for the backprogation algorithm for RNN is called as \"backpropogation through time\". If you unroll the RNN in time the gradient need to be propogated in time! We will start will the loss whihc is sum of the all the loss till time t. Gradients of the cost with respect to  a_t at every time-step  t helps the gradient backpropagate to the previous RNN-cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_rnnCell(da_t_1, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        da_prev_t: The gradient at time t+1 if t is the current time\n",
    "        cache : collection of stored (a<t>,a<t-1>,x<t>,weight_dict)\n",
    "    Output:\n",
    "    gradient_t{\n",
    "        da_t:\n",
    "        dx_t:\n",
    "        dW_aa:\n",
    "        db_a:\n",
    "        dW_ax:\n",
    "        }\n",
    "    \"\"\"\n",
    "    a_cur, a_prev, x_t, parameter = cache \n",
    "    W_ax = parameter['W_ax']\n",
    "    W_aa = parameter['W_aa']\n",
    "    b_a = parameter['b_a']\n",
    "    dtanh = (1-np.tanh(W_ax.dot(x_t)+W_aa.dot(a_prev)+b_a)**2)*(da_t_1)\n",
    "    dx_t =  W_ax.T.dot(dtanh)\n",
    "    da_t = W_aa.T.dot(dtanh)\n",
    "    dW_aa = dtanh.dot(a_prev.T)\n",
    "    dW_ax = dtanh.dot(x_t.T)\n",
    "    db_a = np.sum(dtanh,axis=1,keepdims=True)\n",
    "    gradient_t ={\n",
    "        \"da_t_1\": da_t,\n",
    "        \"dx_t\": dx_t,\n",
    "        \"dW_aa\": W_aa,\n",
    "        \"db_a\":db_a,\n",
    "        \"dW_ax\":dW_ax\n",
    "        }\n",
    "    return gradient_t;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming to the tricky part of backprop. Let da hold are all derivatives of the hidden states over the full sequence for the previous training epoch. When propagating the da_prev of t to the da_next of t-1, we need to add the corresponding da which would be its value in the previous epoch.\n",
    "            <img src=\"cool_rnn.png\" style=\"width:700px;height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_pass(da, caches):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    da : Upstream gradient dimension(n_x, m,T_x )//Calculated using loss \n",
    "    caches : tuple of caches (cache, x)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    (cache, x)  = caches\n",
    "    (a_curr, a_prev, x_curr, weight_dict)= cache[0]\n",
    "    #Initialize the final back variables\n",
    "    ##\n",
    "    n_x,m,T_x = x.shape\n",
    "    n_a = a_curr.shape[0]\n",
    "    dWaa = np.zeros((n_a,n_a)) \n",
    "    dWax = np.zeros((n_a,n_x))\n",
    "    dba = np.zeros((n_a,1))\n",
    "    dx = np.zeros((n_x,m,T_x))\n",
    "    da_prev_t = np.zeros((n_a,m)) \n",
    "    for t in reversed(range(T_x)):\n",
    "        total_back = da_prev_t + da[:,:,t]\n",
    "        gradient_t = backward_rnnCell(total_back,cache=cache[t])\n",
    "        da_prev_t,dx_t,dWaa_t, dba_t, dWax_t = gradient_t[\"da_t_1\"], gradient_t[\"dx_t\"], gradient_t[\"dW_aa\"],gradient_t[\"db_a\"], gradient_t[\"dW_ax\"]\n",
    "        dWaa+= dWaa_t\n",
    "        dWax+= dWax_t\n",
    "        dba+= dba_t\n",
    "        dx[:,:,t] = dx_t \n",
    "    da0 = da_prev_t\n",
    "    final_gradient = {\"dx\":dx,\"dWaa\":dWaa,\"dWax\":dWax,\"dba\":dba,\"da0\":da0}\n",
    "    return final_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
